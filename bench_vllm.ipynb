{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "350341ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "\n",
    "import nvidia\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from vllm import LLM, SamplingParams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "070834fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fb92db2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ac40d823",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "650a983b63054dcf8696281028ba1761",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (â€¦)/configuration_RW.py:   0%|          | 0.00/2.61k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A new version of the following files was downloaded from https://huggingface.co/tiiuae/falcon-7b:\n",
      "- configuration_RW.py\n",
      ". Make sure to double-check they do not contain any added malicious code. To avoid downloading new versions of the code file, you can pin a revision.\n",
      "You are using a model of type RefinedWebModel to instantiate a model of type falcon. This is not supported for all configurations of models and can yield errors.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 08-17 22:11:15 llm_engine.py:70] Initializing an LLM engine with config: model='/mnt/falcon_7b_model_adapter', tokenizer='/mnt/falcon_7b_model_adapter', tokenizer_mode=auto, trust_remote_code=True, dtype=torch.float16, use_dummy_weights=False, download_dir=None, use_np_weights=False, tensor_parallel_size=1, seed=0)\n",
      "INFO 08-17 22:11:23 llm_engine.py:196] # GPU blocks: 43795, # CPU blocks: 32768\n"
     ]
    }
   ],
   "source": [
    "# Load the model to vLLM.\n",
    "llm = LLM(model=\"/mnt/falcon_7b_model_adapter\",trust_remote_code=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f8b58119",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the chat dialogue:\n",
      "Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN ðŸ™Š Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting ðŸ˜± To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on ðŸ˜‚\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: ðŸ™‰\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan ðŸ˜‚\n",
      "Hilary: ðŸ˜Ž\n",
      "Elliot: See you at 2 then xx\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# select a random test sample\n",
    "# sample = test_dataset[randint(0, len(test_dataset))]\n",
    "\n",
    "# Change the index to select a different sample\n",
    "sample = test_dataset[5]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0b7a506",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processed prompts: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1/1 [00:16<00:00, 16.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Hilary is going to lunch. Benjamin might pass by after lunchtime to take the keys from Hilary. Hilary is meeting French people to talk about colonial food and she is going to La Cantina for Italian cuisine afterwards. Elliot will meet their French guests for a talk and Hilary.  Benjamin may join them after 2 pm. They will then be going to La Cantina.  Hilary, Elliot and Benjamin are going to drink a few drinks there. Hilary will be meeting them for 2 o'clock. Benjamin may join them at this time. They plan to take a nap.  They are going on a trip with Elliott after they meet.  They will go to parties in Mexico and write a paper on colonial food together.  Hilary needs to meet with the French guests after talking to Elliot.  They will then join Benjamin for a nap and are going to La Cantina.  Now Hilary has gone to meet the French people.  Hilary will be going to La Cantina after 2. They are going to sit, discuss and drink a lot of alcohol.  Benjamin will join them for the party after La Cantina. Hilary and Elliot are meeting French guests for a talk this evening. Then they will go to a party together. Benjamin is going to join them. \\n---\\nSummary:\\nBenjamin changed his mind about taking the nap. Hilary is going to have lunch. Hilary is meeting the French people. They are going to have drinks after lunch. Hilary, Elliot and Benjamin are going to visit Mexico together.  They want to be at La Cantina at 2 o'clock. Benjamin plans to join them. They will ent at La Cantina to sit and discuss. Then Benjamin will join them for talk and some alcohol. Then they will go to a party together with Hilary, Elliot and the French people.  Hilary will be going to another party with the French on November 13.  Lu Ann will also join them at this point.  After then he will go to a party with Benjamin and Elliot.  They will leave Mexico and go back to Mexico on November 1.  Ellen will also join them as the Christmas party.  They are departing on 4 December, only 12 days left.  Hilary wants to see the Thesis Defence. They are going to Ciencia Frontiers. After that they are going to drink alcohol. Anne and Raffaella are going along. Hilary, Raffaella, Anne and Hohenjo\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "max_tokens = 500\n",
    "\n",
    "sampling_params = SamplingParams(\n",
    "        temperature=1.0,\n",
    "        top_p=1,\n",
    "        max_tokens=max_tokens,\n",
    "    )\n",
    "start_time = time.perf_counter()\n",
    "outputs = llm.generate(test_sample, sampling_params)\n",
    "end_time = time.perf_counter()\n",
    "generated_text = outputs[0].outputs[0].text\n",
    "print(f\"{generated_text!r}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e652bad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " vLLM took 16.914 sec and generated 29.561  tokens/sec\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n vLLM took {round(end_time - start_time, 3)} sec and generated {round(max_tokens / (end_time - start_time),3)}  tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03657e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "outputs = llm.generate(test_sample, sampling_params)\n",
    "generated_text = outputs[0].outputs[0].text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f18e819",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
