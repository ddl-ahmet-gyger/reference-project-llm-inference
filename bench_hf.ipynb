{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "361235e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the dependencies\n",
    "\n",
    "import nvidia\n",
    "import os\n",
    "import time\n",
    "import torch\n",
    "import transformers\n",
    "\n",
    "from datasets import load_dataset\n",
    "from random import randint\n",
    "from transformers import GenerationConfig, AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "33e0def3",
   "metadata": {},
   "outputs": [],
   "source": [
    "cuda_install_dir = '/'.join(nvidia.__file__.split('/')[:-1]) + '/cuda_runtime/lib/'\n",
    "os.environ['LD_LIBRARY_PATH'] =  cuda_install_dir\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "08e4a36b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset from the hub\n",
    "test_dataset = load_dataset(\"samsum\", split=\"test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a60741ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a297aaa5b7a4990949e068c4b297ab5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load the fine tuned falcon-7b model\n",
    "model_id = \"/mnt/falcon_7b_model_adapter\"\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "            load_in_8bit=True,\n",
    "        )\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id, \n",
    "                                             trust_remote_code=True, \n",
    "                                             torch_dtype=torch.float16,\n",
    "                                             device_map=\"auto\",\n",
    "                                             quantization_config=bnb_config,\n",
    "                                             cache_dir='/mnt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "133afa49",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the chat dialogue:\n",
      "Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN ðŸ™Š Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting ðŸ˜± To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on ðŸ˜‚\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: ðŸ™‰\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan ðŸ˜‚\n",
      "Hilary: ðŸ˜Ž\n",
      "Elliot: See you at 2 then xx\n",
      "---\n",
      "Summary:\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Change the index to select a different sample\n",
    "sample = test_dataset[5]\n",
    "\n",
    "# format sample\n",
    "prompt_template = f\"Summarize the chat dialogue:\\n{{dialogue}}\\n---\\nSummary:\\n\"\n",
    "\n",
    "test_sample = prompt_template.format(dialogue=sample[\"dialogue\"])\n",
    "\n",
    "print(test_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2243f8f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summarize the chat dialogue:\n",
      "Benjamin: Hey guys, what are we doing with the keys today?\n",
      "Hilary: I've got them. Whoever wants them can meet me at lunchtime or after\n",
      "Elliot: I'm ok. We're meeting for the drinks in the evening anyway and I guess we'll be going back to the apartment together?\n",
      "Hilary: Yeah, I guess so\n",
      "Daniel: I'm with Hilary atm and won't let go of her for the rest of the day, so any option you guys choose is good for me\n",
      "Benjamin: Hmm I might actually pass by at lunchtime, take the keys and go take a nap. I'm sooo tired after yesterday\n",
      "Hilary: Sounds good. We'll be having lunch with some French people (the ones who work on the history of food in colonial Mexico - I already see you yawning your head off)\n",
      "Benjamin: YAAAAWN ðŸ™Š Where and where are you meeting?\n",
      "Hilary: So I'm meeting them at the entrance to the conference hall at 2 pm and then we'll head to this place called La Cantina. Italian cuisine, which is quite funny, but that's what they've chosen\n",
      "Benjamin: Interesting ðŸ˜± To be honest, Hilary, I almost feel like changing my mind. Wanting to take this nap might end up costing me to dear\n",
      "Hilary: Oh come on ðŸ˜‚\n",
      "Benjamin: All these terrible obstacles on mu way to bed might just prove to much to take\n",
      "Hilary: We'll try to avoid talking about their subject of research. Oh wait, no, I'm actually meeting them because I wanted to chat about their research lol\n",
      "Elliot: ðŸ™‰\n",
      "Hilary: Do join us, we're going to have fun. And then you'll take the keys and take this most deserved of naps\n",
      "Elliot: Sounds like a plan ðŸ˜‚\n",
      "Hilary: ðŸ˜Ž\n",
      "Elliot: See you at 2 then xx\n",
      "---\n",
      "Summary:\n",
      "Benjamin is tired after yesterday. Hilary is meeting French people at 2 pm at the entrance to the conference hall. They will go to La Cantina for Italian cuisine. Hilary wants to talk about their research. Hilary will take the keys and take a\n"
     ]
    }
   ],
   "source": [
    "#generate output from the fine tuned falcon-7b model to compare run time\n",
    "#set the tokens for the summary evaluation\n",
    "tokenizer = transformers.AutoTokenizer.from_pretrained(\"tiiuae/falcon-7b\")\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "start_time = time.perf_counter()\n",
    "input_ids = tokenizer(test_sample, return_tensors=\"pt\").input_ids\n",
    "input_ids = input_ids.to('cuda')\n",
    "max_length = 500\n",
    "\n",
    "generation_config = GenerationConfig(\n",
    "            pad_token_id=tokenizer.pad_token_id,\n",
    "            max_length=max_length,\n",
    "        )\n",
    "\n",
    "with torch.no_grad():\n",
    "    generated_ids = model.generate(input_ids, generation_config=generation_config)\n",
    "    \n",
    "gen_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "end_time = time.perf_counter()     \n",
    "print(gen_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a43c59a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Huggingface finetuned model took 7.794 sec and generated 64.154 tokens/sec\n"
     ]
    }
   ],
   "source": [
    "print(f'\\n Huggingface finetuned model took {round(end_time - start_time, 3)} sec and generated {round(max_length / (end_time - start_time),3)} tokens/sec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "04ab8db9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.79 s Â± 4.29 ms per loop (mean Â± std. dev. of 30 runs, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "%%timeit -n 1 -r 30\n",
    "with torch.no_grad():\n",
    "      outputs = model.generate(inputs=input_ids, generation_config=generation_config)  \n",
    "gen_text = tokenizer.batch_decode(generated_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b99c067d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
